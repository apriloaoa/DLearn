__author__ = 'CUPL'
import numpy as np

# true model
def f(X, std):


    return np.sum(X[:, :2], axis=1) + std*np.random.normal(size=len(X))
    #return np.sum(np.sin(X[:, :2]), axis=1) + std*np.random.normal(size=len(X))#
    #return np.sin(np.sum(X[:, :2], axis=1)) + std*np.random.normal(size=len(X))

    #return np.prod(X[:, :2], axis=1) + std * np.random.normal(size=len(X))
    #return np.prod(np.sin(X[:, :2]), axis=1) + std*np.random.normal(size=len(X))
    #return np.sin(np.prod(X[:, :2], axis=1)) + std*np.random.normal(size=len(X))
    #return np.sin(np.sqrt(np.sum(X[:, :2]**2, axis=1))) + std*np.random.normal(size=len(X))

    #return np.sum(np.cos(X[:, :2]), axis=1) + std*np.random.normal(size=len(X))
    #return np.sum(X[:, :2]**2, axis=1) + std * np.random.normal(size=len(X))

# Generate training data
def data_generator(f, n, p):  #sample size = n and dimension = p
    X_train = np.random.normal(scale=1, size=[n, p])
    Y_train = f(X_train, std=0.1)
    return X_train, Y_train




if __name__ == '__main__':
    n_train = 100  # sample size
    n_test = 10
    p = 2  # dimension of features

    X_train, Y_train = data_generator(f, n_train, p)  # generate training data from f(X)
    X_test, Y_test = data_generator(f, n_test, p)  # generate testing data from f(X)

[1.4672027420608256, 1.1624084497761451, 1.111465156674295, 1.098158252506606, 1.1014032982747393, 1.0966442879948395, 1.0939809802464355, 1.0940916790710111, 1.0961158000776063, 1.0999077287005468, 1.092590717114835, 1.1043795027769994, 1.0957028029639886, 1.0970314499055247, 1.090080942053151, 1.1044052388902299, 1.0966590854067135, 1.1033464763705616, 1.0963809182858999, 1.0978434419108174, 1.1095928378467979, 1.0988925948239399, 1.1025748159535327, 1.1029427208544853, 1.0973250928608513, 1.1001754095736129, 1.1077908607113713, 1.1016213818165397, 1.0954620468396896, 1.1071571012640702, 1.0992117438204896, 1.103192915105675, 1.1047032624650566, 1.0987008132042901, 1.104470697848678, 1.0930616519097003, 1.103802255594162, 1.0982433103588838, 1.0913146007604237, 1.094764985040668, 1.0958537067173242, 1.0928845728562511, 1.0936628615072919, 1.1019164038729024, 1.103205384331986, 1.103283922138947, 1.1040421382265753, 1.1019317661214845, 1.1083572435216846, 1.1116358393058425, 1.1007895428354473, 1.1001580957729042, 1.0991305379401062, 1.0969533617487488, 1.1098659263037733, 1.1074237648828233, 1.1078859527644271, 1.0994874776636456, 1.1043814968385173, 1.0963392048251617, 1.0879573509797702, 1.0886866795136543, 1.0898055433047387, 1.0990991150734186, 1.1051588333294202, 1.1004115432179675, 1.1064704832279946, 1.1013111015015469, 1.1060202687194041, 1.1121288608494333, 1.0987213160380185, 1.1028748414923484, 1.1043474546334824, 1.1020421877699977, 1.0977893185241479, 1.1188329603491349, 1.0924177429106643, 1.1055041190658104, 1.1044802235796192, 1.1016787949345999, 1.1047858874496652, 1.1060345091698318, 1.0999030529009399, 1.1036638544705135, 1.1074056682788052, 1.1033852620888998, 1.109851095283982, 1.1028577806736635, 1.1005554933426898, 1.0984815347507979, 1.101712072100647, 1.1057836461286248, 1.1154966714791661, 1.1122038593586143, 1.1010661804770265, 1.1003717750229802, 1.0967746370115652, 1.0972081496059471, 1.1106040337395453, 1.1038909891845639, 1.1125889787387619]
[0.22808304896919046, 0.17112917992032384, 0.16131956339582945, 0.16255205437777162, 0.16442034132842748, 0.15667742986634223, 0.16549951108246738, 0.16192750168784711, 0.15810741607816084, 0.16072331731721243, 0.15583671497374488, 0.16354051265725411, 0.15650798169311095, 0.16413234592755685, 0.16447986229776906, 0.16129900902568273, 0.15827967027253459, 0.15759071488726545, 0.15599910274783949, 0.16125312269959871, 0.16108907230794842, 0.16199110972672109, 0.16190875378253133, 0.15646054731825115, 0.16676600123490595, 0.15398061548435074, 0.16556991014189071, 0.15941273068695833, 0.16419656298731386, 0.1667777789020915, 0.1607757974108035, 0.16158915791433817, 0.15717897089102537, 0.1547224865122066, 0.15746904742844664, 0.16345305012745129, 0.15893050459788211, 0.16264884672111299, 0.16577681442484202, 0.16562601909864622, 0.15826958945570097, 0.15406556559800502, 0.15675121818897611, 0.16484483491839372, 0.15870213353302898, 0.16144411761714425, 0.16833916718848191, 0.15568984449226508, 0.15945145940485123, 0.15955896677548351, 0.1550124207319197, 0.16017366429681756, 0.15375134017985564, 0.16208733433449465, 0.16252332466598957, 0.16433399484188577, 0.16046420426327207, 0.15668263348422501, 0.16378463950157071, 0.16535087890767347, 0.16020171822992929, 0.16071616836607402, 0.15574196730135617, 0.15507366425148253, 0.15816335620029362, 0.16154134011649823, 0.16344560574876674, 0.1611980505170483, 0.15313124487097363, 0.16626134418580482, 0.15752342029761049, 0.16324361685570016, 0.16719932610137989, 0.1592569068746065, 0.15729880429441787, 0.16207049360363834, 0.16321479691012253, 0.15368868020938517, 0.16451498629983136, 0.16224872888284581, 0.16494093601932944, 0.1653946464474767, 0.15855803651179085, 0.15884558883693109, 0.16529108226675684, 0.16103351760468546, 0.15805498561316206, 0.15828458235487636, 0.16072336039807852, 0.15912924989787405, 0.15647437716999765, 0.15767960891056854, 0.16159996721082928, 0.15734500334404744, 0.1526450692919023, 0.1615107980483185, 0.15321049501244371, 0.15721831969646255, 0.1625050204561829, 0.15694209039981982, 0.16764914552153168]




